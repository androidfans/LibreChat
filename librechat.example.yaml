version: 1.1.4
cache: true
endpoints:
  custom:
    - name: "OpenRouter"
      # For `apiKey` and `baseURL`, you can use environment variables that you define.
      # recommended environment variables:
      apiKey: "${OPENROUTER_KEY}" # NOT OPENROUTER_API_KEY
      # baseURL: "http://host.docker.internal:39527/v1" # 可改为你的自建/代理 OpenAI 兼容服务
      baseURL: "https://openrouter.ai/api/v1"
      models:
        default: [
          "google/gemini-2.5-pro",
          "anthropic/claude-sonnet-4.5",
          "openai/gpt-4.1",
        ]
        fetch: true # 走api获取模型列表; 若服务端不支持可设为 false
      titleConvo: true  # 是否生成对话标题
      titleModel: "openai/gpt-4.1" # 用来生成标题的模型, 不要用 Gemini-2.0-Flash模型 在默认prompt下会返回多余的 related search, 导致UI显示异常
      # titleMethod: "completion" # function 或 completion, function是调用函数生成标题, completion是解析对话用模型生成标题
      summarize: true
      summaryModel: "openai/gpt-4.1"
      # forcePrompt: false # 一般false; 为true时 1) 请求 /completion 而不是 /chat/completion 2) 请求体 messages 改为 prompt(所有消息合并)
      # titleMessageRole: "system" # 生成标题的请求role值, 默认为system; 若模型会忽略system消息可改为user
      # addParams: # 服务商需要的额外参数
        # endpoint_api_specific_key: ""
      # Recommended: Drop the stop parameter from the request as Openrouter models use a variety of stop tokens.
      dropParams: ["stop"]
      # headers: # 额外的请求头
        # Content-Type: "application/json"
      # directEndpoint: false # 为true的话, 直接使用这个endpoint的url, 不会拼接成 /chat/completion 的url, 例如: http://host.docker.internal:39527/v1/chat/completion
      modelDisplayLabel: "OpenRouter"

    - name: "laitool-mjrelax"
      apiKey: "${LAITOOL_MJRELAX_KEY}"
      baseURL: "https://laitool.cc/v1/"
      models:
        default: [
          "deepseek-chat",
          "gemini-2.5-flash",
          "gemini-2.5-pro",
        ]
        fetch: false
      titleConvo: true
      titleModel: "deepseek-chat"
      summarize: true
      summaryModel: "deepseek-chat"
      modelDisplayLabel: "来推"
   
    - name: "laitool-official"
      apiKey: "${LAITOOL_OFFICIAL_KEY}"
      baseURL: "https://laitool.cc/v1/"
      models:
        default: [
          "claude-sonnet-4-5-20250929",
          "claude-sonnet-4-5-20250929-thinking", 
          "gpt-4.1-mini"
        ]
        fetch: true # fetching list of models is not supported
      titleConvo: true
      titleModel: "gpt-4.1-mini"
      summarize: true
      summaryModel: "gpt-4.1-mini"
      modelDisplayLabel: "来推官转"

    - name: "copilot"
      apiKey: "dummy"
      baseURL: "http://host.docker.internal:4141/v1/"
      models:
        default: [
          "gemini-3-pro-preview",
          "gemini-2.5-pro",
          "gpt-4.1",
          "claude-sonnet-4.5",
        ]
        fetch: true
      titleConvo: true
      titleModel: "gpt-4.1"
      summarize: true
      summaryModel: "gpt-4.1"
      modelDisplayLabel: "Copilot逆向API"
 
# modelSpecs节点存在，UI将不再显示预置模型和上面定义的default models, 仅显示model_specs中定义的模型和 addedEndpoints 的endpoints
modelSpecs:
  # 配置说明 https://www.librechat.ai/docs/configuration/librechat_yaml/object_structure/model_specs
  enforce: false  # 设置为true, 只能使用下面 list 的模型，从二级菜单选择的模型无法正常使用
  prioritize: true # 名字相同的情况下, 覆盖预置模型
  addedEndpoints: # 除了下面定义的模型, 以二级菜单模式显示的端点的模型
    # 预置 可用 value： agents, openAI, azureOpenAI, google, anthropic, assistants, azureAssistants, bedrock
    # agents 是助手, 不写就看不见助手了
    - agents
    - smallai
    - OpenRouter
    - laitool-mjrelax 
    - laitool-official 
    - google-official
    - copilot
  list:
    - name: "laitool-deepseek-chat"
      label: "来推 DeepSeek Chat"
      iconURL: "https://laitool.cc/favicon.ico"
      description: "deepseek-chat"
      preset:
        endpoint: "laitool-mjrelax"
        model: "deepseek-chat"
        maxContextTokens: 99999999
   
    - name: "laitool-gemini-2.5-flash"
      label: "来推 Gemini-2.5-flash"
      iconURL: "https://laitool.cc/favicon.ico"
      description: "gemini-2.5-flash"
      preset:
        endpoint: "laitool-mjrelax"
        model: "gemini-2.5-flash"
        maxContextTokens: 99999999
   
    - name: "laitool-gemini-2.5-pro"
      label: "来推 Gemini-2.5-pro"
      iconURL: "https://laitool.cc/favicon.ico"
      description: "gemini-2.5-pro"
      preset:
        endpoint: "laitool-mjrelax"
        model: "gemini-2.5-pro"
        maxContextTokens: 99999999
   
    - name: "laitool-claude-3-7-sonnet-20250219"
      label: "来推官转 Claude 3.7 Sonnet"
      iconURL: "https://laitool.cc/favicon.ico"
      description: "claude-3-7-sonnet-20250219"
      preset:
        endpoint: "laitool-official"
        model: "claude-3-7-sonnet-20250219"
        maxContextTokens: 99999999

    - name: "copilot-gemini-2.5-pro"
      label: "Copilot Gemini 2.5 Pro"
      iconURL: "https://self-host-cdn.s3.bitiful.net/github-copilot-icon-white.webp"
      description: ""
      preset:
        endpoint: "copilot"
        model: "gemini-2.5-pro"
        maxContextTokens: 99999999

    - name: "copilot-gemini-3-pro-preview"
      label: "Copilot Gemini 3 Pro"
      iconURL: "https://self-host-cdn.s3.bitiful.net/github-copilot-icon-white.webp"
      description: ""
      preset:
        endpoint: "copilot"
        model: "gemini-3-pro-preview"
        maxContextTokens: 99999999

    - name: "copilot-gemini-3-flash-preview"
      label: "Copilot Gemini 3 flash"
      iconURL: "https://self-host-cdn.s3.bitiful.net/github-copilot-icon-white.webp"
      description: ""
      preset:
        endpoint: "copilot"
        model: "gemini-3-flash-preview"
        maxContextTokens: 99999999

    - name: "copilot-claude-sonnet-4.5"
      label: "Copilot Claude Sonnet 4.5"
      iconURL: "https://self-host-cdn.s3.bitiful.net/github-copilot-icon-white.webp"
      description: ""
      preset:
        endpoint: "copilot"
        model: "claude-sonnet-4.5"
        maxContextTokens: 99999999

    - name: "copilot-claude-opus-4.5"
      label: "Copilot Claude Opus 4.5"
      iconURL: "https://self-host-cdn.s3.bitiful.net/github-copilot-icon-white.webp"
      description: ""
      preset:
        endpoint: "copilot"
        model: "claude-opus-4.5"
        maxContextTokens: 99999999
